# ğŸš¢ Titanic Decision Tree 

Este repositÃ³rio contÃ©m um modelo de **Machine Learning** que utiliza uma **Ãrvore de DecisÃ£o** para prever a sobrevivÃªncia dos passageiros do Titanic.

## ğŸ“Œ Sobre o Projeto
O projeto inclui:

### ğŸ“Š **Carregamento e Tratamento de Dados**
âœ” Carrega o dataset Titanic 
âœ” Trata valores nulos (`Age` com a mÃ©dia, remove `Embarked` nulo)  
âœ” Converte variÃ¡veis categÃ³ricas em nÃºmeros   
âœ” Seleciona as colunas que serÃ£o usadas para prever a sobrevivÃªncia  
âœ” Divide os dados em treino e teste (80% / 20%)   

###  **Treinamento da Ãrvore de DecisÃ£o**
âœ” Cria uma Ã¡rvore de decisÃ£o com `max_depth=20`   
âœ” Treina o modelo com os dados de treino  
âœ” Calcula a acurÃ¡cia do modelo nos dados de treino   

###  **AvaliaÃ§Ã£o do Modelo**
âœ” Avalia o modelo nos dados de teste (dados novos)   
âœ” Faz previsÃµes para os passageiros de teste   
âœ” Calcula a acurÃ¡cia manualmente usando `accuracy_score` 

###  **VisualizaÃ§Ã£o da Estrutura da Ãrvore**
âœ” Exibe a estrutura da Ã¡rvore de decisÃ£o graficamente 

---

## ğŸ›  Tecnologias Utilizadas
- **Python** 
- **Pandas** 
- **Scikit-Learn** 
- **Matplotlib** 
- **Seaborn** 

---

## ğŸš€ Como Executar

1. **Instale as Dependencias:**
   ```bash
   pip install pandas scikit-learn matplotlib seaborn

2. **Execute o script:**
   ```bash
   python nome_do_script.py

##  Resumo do Modelo 
ğŸ”¹ O modelo atual traz um overfitting (97.89% no treino vs. 79.89% no teste).

##  Melhorias Futuras
ğŸ”¹ Reduzir a profundidade da Ã¡rvore (max_depth=5) para melhorar a generalizaÃ§Ã£o.
ğŸ”¹ Ajustar hiperparÃ¢metros (min_samples_split, min_samples_leaf) para evitar overfitting.
ğŸ”¹ Testar outros modelos, como Random Forest ou XGBoost, para melhorar o desempenho.

##   ContribuiÃ§Ã£o
Se quiser contribuir, fique Ã  vontade para abrir um Pull Request ou sugerir melhorias. 


